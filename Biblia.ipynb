{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:42.924142Z",
     "start_time": "2025-05-15T18:30:42.921844Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ec85c14e4be8cd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.205316Z",
     "start_time": "2025-05-15T18:30:42.934211Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\854068287.py:15: DtypeWarning: Columns (1,2,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cs_1 = pd.read_csv('{}.csv'.format(CS1),encoding='latin-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cs_2.csv no encontrado. Se omitirá.\n",
      "Archivo cs_3.csv no encontrado. Se omitirá.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\854068287.py:32: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_gec = pd.read_csv('Catalogo_GEC_RTM.csv')\n"
     ]
    }
   ],
   "source": [
    "#Catálogos de OrderEntry y Reparto\n",
    "\n",
    "UO = 'MR0068'\n",
    "CS1 = 'MJCQ'\n",
    "CS2 = ''\n",
    "CS3 = ''\n",
    "\n",
    "cat_oe = pd.read_csv('Catalogo_OE.csv')\n",
    "cat_rep = pd.read_csv('Catalogo_Reparto.csv')\n",
    "\n",
    "#Base de datos Clientes SQL\n",
    "planner = pd.read_csv('Planner_{}.csv'.format(UO))\n",
    "\n",
    "#Bases de datos Greenmile\n",
    "cs_1 = pd.read_csv('{}.csv'.format(CS1),encoding='latin-1')\n",
    "\n",
    "try:\n",
    "    cs_2 = pd.read_csv('{}.csv'.format(CS2), encoding='latin-1')\n",
    "except FileNotFoundError:\n",
    "    print(\"Archivo cs_2.csv no encontrado. Se omitirá.\")\n",
    "    cs_2 = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    cs_3 = pd.read_csv('{}.csv'.format(CS3), encoding='latin-1')\n",
    "except FileNotFoundError:\n",
    "    print(\"Archivo cs_3.csv no encontrado. Se omitirá.\")\n",
    "    cs_3 = pd.DataFrame()\n",
    "\n",
    "greenmile = pd.concat([cs_1,cs_2,cs_3], ignore_index=True)\n",
    "\n",
    "#Base de GEC RTM\n",
    "df_gec = pd.read_csv('Catalogo_GEC_RTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bb6da487d7ff7cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.235037Z",
     "start_time": "2025-05-15T18:30:43.228341Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seleccionar y renombrar columnas relevantes\n",
    "greenmile = greenmile[['CÃ³digo de la UbicaciÃ³n', 'Longitud', 'Latitud', 'OrganizaciÃ³n']].copy()\n",
    "greenmile.columns = ['Cliente GM', 'Longitud GM', 'Latitud GM', 'UO GM']\n",
    "\n",
    "# Rellenar valores nulos en coordenadas con 0\n",
    "greenmile['Longitud GM'] = greenmile['Longitud GM'].fillna(0)\n",
    "greenmile['Latitud GM'] = greenmile['Latitud GM'].fillna(0)\n",
    "\n",
    "# Convertir 'Cliente' a numérico, eliminando filas con errores\n",
    "greenmile['Cliente GM'] = pd.to_numeric(greenmile['Cliente GM'], errors='coerce')\n",
    "greenmile = greenmile.dropna(subset=['Cliente GM'])\n",
    "greenmile['Cliente GM'] = greenmile['Cliente GM'].astype(int)\n",
    "\n",
    "greenmile = greenmile.drop_duplicates(subset='Cliente GM', keep='first')\n",
    "greenmile = greenmile.drop_duplicates(subset='Latitud GM')\n",
    "greenmile = greenmile.drop_duplicates(subset='Longitud GM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c01bb3f83156bbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.307065Z",
     "start_time": "2025-05-15T18:30:43.254024Z"
    }
   },
   "outputs": [],
   "source": [
    "planner['DIRECCIÓN'] = planner['DIRECCIÓN'].fillna('DESCONOCIDO')\n",
    "planner['ENTRE_CALLE_1'] = planner['ENTRE_CALLE_1'].fillna('DESCONOCIDO')\n",
    "planner['ENTRE_CALLE_2'] = planner['ENTRE_CALLE_2'].fillna('DESCONOCIDO')\n",
    "planner['DELEGACION_MUNICIPIO'] = planner['DELEGACION_MUNICIPIO'].fillna('DESCONOCIDO')\n",
    "planner['ID_VP'] = planner['ID_VP'].fillna('SIN VP')\n",
    "planner['MODALIDAD'] = planner['MODALIDAD'].fillna('NO DATA')\n",
    "planner['TPV'] = planner['TPV'].fillna('NO DATA')\n",
    "planner['RUTA'] = planner['RUTA'].fillna('NO DATA')\n",
    "planner['FV'] = planner['FV'].fillna('NO DATA')\n",
    "planner['RITMO'] = planner['RITMO'].fillna('NO DATA')\n",
    "planner['MÉTODO_VENTA'] = planner['MÉTODO_VENTA'].fillna('NO DATA')\n",
    "planner['ID_MULTICENTRO'] = planner['ID_MULTICENTRO'].fillna('NO ASIGNADO')\n",
    "\n",
    "df_gec_renamed = df_gec.rename(columns={'CLIENTE': 'ID_SAP'})\n",
    "planner = pd.merge(planner, right=df_gec_renamed, how='left', on='ID_SAP')\n",
    "\n",
    "# Rellenamos los valores faltantes del GEC nuevo (por ejemplo, GEC_RTM) con el GEC anterior\n",
    "planner['GEC_RTM'] = planner['GEC_RTM'].fillna(planner['GEC_SAP'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fcd380ebef0ee0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.389170Z",
     "start_time": "2025-05-15T18:30:43.325138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Columnas duplicadas encontradas: ['Ruta ZGC', 'Fv ZGC', 'Ritmo ZGC', 'Método_venta ZGC']\n",
      "✅ Columnas duplicadas eliminadas.\n",
      "      ID_SAP                         CLIENTE  FECHA_ALTA    DIRECCIÓN  \\\n",
      "0  660603349              Ramon Zavala Loeza  05.11.2011  DESCONOCIDO   \n",
      "1  660603366      Luis Alberto Vargas Zurita  05.11.2011  DESCONOCIDO   \n",
      "2  670001322  MANUEL ALEJANDRO GARCIA GODINE  05.11.2011  DESCONOCIDO   \n",
      "3  670001845           Miguel Alcaraz Chavez  05.11.2011  DESCONOCIDO   \n",
      "4  670001848          Roberto Mejía Cisneros  05.11.2011  DESCONOCIDO   \n",
      "\n",
      "  ENTRE_CALLE_1 ENTRE_CALLE_2 COLONIA DELEGACION_MUNICIPIO  CODIGO_POSTAL  \\\n",
      "0   DESCONOCIDO   DESCONOCIDO     NaN          DESCONOCIDO          58200   \n",
      "1   DESCONOCIDO   DESCONOCIDO     NaN          DESCONOCIDO          58200   \n",
      "2   DESCONOCIDO   DESCONOCIDO     NaN          DESCONOCIDO          58200   \n",
      "3   DESCONOCIDO   DESCONOCIDO     NaN          DESCONOCIDO          58200   \n",
      "4   DESCONOCIDO   DESCONOCIDO     NaN          DESCONOCIDO          58200   \n",
      "\n",
      "  ESTATUS  ... Fv ZGG ZGG_L ZGG_M ZGG_R ZGG_J ZGG_V  ZGG_S ZGG_D  Ritmo ZGG  \\\n",
      "0       Z  ...      -     0     0     0     0     0      0     0          -   \n",
      "1       Z  ...      -     0     0     0     0     0      0     0          -   \n",
      "2       Z  ...      -     0     0     0     0     0      0     0          -   \n",
      "3       Z  ...      -     0     0     0     0     0      0     0          -   \n",
      "4       A  ...      -     0     0     0     0     0      0     0          -   \n",
      "\n",
      "  Método_venta ZGG  \n",
      "0                -  \n",
      "1                -  \n",
      "2                -  \n",
      "3                -  \n",
      "4                -  \n",
      "\n",
      "[5 rows x 222 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
      "C:\\Users\\MX05081363\\AppData\\Local\\Temp\\ipykernel_31180\\1562154128.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_final.insert(loc=idx + i, column=col_name, value=serie)\n"
     ]
    }
   ],
   "source": [
    "columnas_basicas = [\n",
    "    'ID_SAP', 'CLIENTE', 'FECHA_ALTA', 'DIRECCIÓN', 'ENTRE_CALLE_1', 'ENTRE_CALLE_2','COLONIA',\n",
    "    'DELEGACION_MUNICIPIO', 'CODIGO_POSTAL', 'ESTATUS', 'ID_VP', 'MODALIDAD',\n",
    "    'GEC_SAP','GEC_RTM','_MADUREZ_CTE','Estatus_Cte', 'CADENA', 'DESC_CADENA','CANAL_KOF','CANAL_SAP', 'GRUPO_RM1', 'GRUPO_RM2',\n",
    "    'ZONA', 'REGIÓN', 'REGIÓN_KOF_CVE', 'UO', 'CENTRO','GERENCIA','JEFATURA',\n",
    "    'CENTRO_SUMINISTRO','CONDICIÓN_EXPEDICIÓN','TIPO_SERVICIO','ID_MULTICENTRO', 'COORD_X', 'COORD_Y'\n",
    "]\n",
    "columnas_tpv = ['ID_SAP', 'TPV', 'RUTA', 'FV', 'RITMO', 'MÉTODO_VENTA']\n",
    "df_tpv = planner[columnas_tpv]\n",
    "df_basico = planner[columnas_basicas].drop_duplicates(subset='ID_SAP')\n",
    "\n",
    "df_basico = df_basico.drop_duplicates(subset='ID_SAP')\n",
    "\n",
    "# Pivot con múltiples valores\n",
    "df_pivot = df_tpv.pivot_table(index='ID_SAP',\n",
    "                          columns='TPV',\n",
    "                          values=['RUTA', 'FV', 'RITMO', 'MÉTODO_VENTA'],\n",
    "                          aggfunc='first')\n",
    "\n",
    "# Aplanar columnas\n",
    "df_pivot.columns = [f'{col[0].capitalize()} {col[1]}' for col in df_pivot.columns]\n",
    "\n",
    " # Reordenar columnas por TPV preferido\n",
    "tpv_preferido = ['ZPV', 'ZTK', 'ZJV','ZTV','ZGH','ZGC','ZGF','ZPG','ZTJ','ZED','ZIV','ZDD','ZGC','ZHB','ZTF','ZVE','ZVS','ZDY']\n",
    "columnas_validas = [col for col in df_pivot.columns if ' ' in col]\n",
    "tpvs_encontrados = {col.split(' ')[1] for col in columnas_validas}\n",
    "otros_tpv = sorted(tpvs_encontrados - set(tpv_preferido))\n",
    "nuevo_orden = tpv_preferido + otros_tpv\n",
    "\n",
    " # Variables esperadas\n",
    "variables = ['Ruta', 'Fv', 'Ritmo', 'Método_venta']\n",
    "orden_columnas = []\n",
    "for tpv in nuevo_orden:\n",
    "    for var in variables:\n",
    "        col_name = f'{var} {tpv}'\n",
    "        if col_name in df_pivot.columns:\n",
    "            orden_columnas.append(col_name)\n",
    "\n",
    "df_pivot = df_pivot[orden_columnas]\n",
    "df_pivot = df_pivot.reset_index()\n",
    "\n",
    "df_pivot = df_pivot.drop_duplicates(subset='ID_SAP')\n",
    "\n",
    "# Reemplazar NaN con guion medio\n",
    "df_pivot = df_pivot.fillna('-')\n",
    "\n",
    "df_pivot = df_pivot.drop_duplicates(subset='ID_SAP')\n",
    "df_basico = df_basico.drop_duplicates(subset='ID_SAP')\n",
    "\n",
    "# Unir datos básicos con pivot\n",
    "df_final = pd.merge(df_basico, df_pivot, on='ID_SAP', how='left')\n",
    "\n",
    "# Eliminar columnas duplicadas (conservando la primera)\n",
    "duplicados = df_final.columns[df_final.columns.duplicated()]\n",
    "if len(duplicados) > 0:\n",
    "    print(\"❌ Columnas duplicadas encontradas:\", duplicados.tolist())\n",
    "    df_final = df_final.loc[:, ~df_final.columns.duplicated()]\n",
    "    print(\"✅ Columnas duplicadas eliminadas.\")\n",
    "else:\n",
    "    print(\"✅ No se encontraron columnas duplicadas.\")\n",
    "\n",
    "# Días de la semana\n",
    "dias_semana = ['L', 'M', 'R', 'J', 'V', 'S', 'D']\n",
    "\n",
    "# Columnas FV por TPV\n",
    "columnas_fv = [col for col in df_final.columns if col.startswith('Fv ')]\n",
    "\n",
    "# Insertar columnas nuevas justo después de cada columna Fv\n",
    "for col_fv in columnas_fv:\n",
    "    tpv = col_fv.split(' ')[1]  # Ej. 'ZGC'\n",
    "    \n",
    "    # Generar nuevas columnas\n",
    "    nuevas_columnas = {}\n",
    "    for dia in dias_semana:\n",
    "        nueva_col = f'{tpv}_{dia}'\n",
    "        nuevas_columnas[nueva_col] = df_final[col_fv].apply(lambda x: 1 if dia in str(x) else 0)\n",
    "    \n",
    "    # Encontrar la posición donde está la columna 'Fv <TPV>'\n",
    "    idx = df_final.columns.get_loc(col_fv) + 1\n",
    "    \n",
    "    # Insertar las columnas en esa posición\n",
    "    for i, (col_name, serie) in enumerate(nuevas_columnas.items()):\n",
    "        df_final.insert(loc=idx + i, column=col_name, value=serie)\n",
    "\n",
    "# Mostrar para validar\n",
    "print(df_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c437eb8ac84a10bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.420866Z",
     "start_time": "2025-05-15T18:30:43.407305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Cliente GM, Longitud GM, Latitud GM, UO GM]\n",
      "Index: []\n",
      "Total de duplicados: 0\n"
     ]
    }
   ],
   "source": [
    "df_coordenadas = pd.merge(left = df_final, right = greenmile, how = 'left', left_on = 'ID_SAP', right_on = 'Cliente GM')\n",
    "\n",
    "df_coordenadas.drop(columns = ['Cliente GM','UO GM'], inplace = True)\n",
    "\n",
    "duplicados_gm = greenmile[greenmile.duplicated(subset='Cliente GM', keep=False)]\n",
    "print(duplicados_gm.sort_values('Cliente GM'))\n",
    "print(f'Total de duplicados: {duplicados_gm[\"Cliente GM\"].nunique()}')\n",
    "\n",
    "assert df_coordenadas['ID_SAP'].is_unique, \"¡Aún hay duplicados en ID_SAP!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff4ead09361926a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.445035Z",
     "start_time": "2025-05-15T18:30:43.441257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Crear la nueva columna \"Latitud Final\" en el DataFrame df_coordenadas\n",
    "df_coordenadas['Latitud Final'] = np.where(\n",
    "    (df_coordenadas['Latitud GM'] == 0) | (df_coordenadas['Latitud GM'].isna()),\n",
    "    df_coordenadas['COORD_Y'],\n",
    "    df_coordenadas['Latitud GM']\n",
    ")\n",
    "\n",
    "# Crear la nueva columna \"Latitud Final\" en el DataFrame df_coordenadas\n",
    "df_coordenadas['Longitud Final'] = np.where(\n",
    "    (df_coordenadas['Longitud GM'] == 0) | (df_coordenadas['Longitud GM'].isna()),\n",
    "    df_coordenadas['COORD_X'],\n",
    "    df_coordenadas['Longitud GM']\n",
    ")\n",
    "\n",
    "df_coordenadas.drop(columns = ['Latitud GM','Longitud GM','COORD_Y','COORD_X'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e316a80aa9492b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.494595Z",
     "start_time": "2025-05-15T18:30:43.490222Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_oe = cat_oe[['Ruta','Tipo','Descripción Tipo']].copy()\n",
    "cat_rep = cat_rep[['RutaReparto','EsquemaReparto']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e06989a0ab59a06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.536758Z",
     "start_time": "2025-05-15T18:30:43.534746Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_por_order_entry(left_df, right_df, claves_left, clave_right='Ruta'):\n",
    "    resultado = left_df.copy()\n",
    "    merged_cols = [col for col in right_df.columns if col != clave_right]\n",
    "    resultado[merged_cols] = None  # Inicializa columnas vacías\n",
    "\n",
    "    sin_match = pd.Series([True] * len(resultado))\n",
    "\n",
    "    for clave in claves_left:\n",
    "        temp_merge = left_df[sin_match].merge(right_df, left_on=clave, right_on=clave_right, how='left')\n",
    "        for col in merged_cols:\n",
    "            resultado.loc[sin_match, col] = temp_merge[col].values\n",
    "        sin_match = resultado[merged_cols[0]].isna()  # puedes mejorar esto si quieres validar múltiples columnas\n",
    "\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d522806f6551fb12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.567482Z",
     "start_time": "2025-05-15T18:30:43.553671Z"
    }
   },
   "outputs": [],
   "source": [
    "df_match_oe = merge_por_order_entry(df_coordenadas, cat_oe, ['Ruta ZPV', 'Ruta ZIV', 'Ruta ZED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46ea5a1c41c08787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.586983Z",
     "start_time": "2025-05-15T18:30:43.585037Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_por_reparto(left_df, right_df, claves_left, clave_right='RutaReparto'):\n",
    "    resultado = left_df.copy()\n",
    "    merged_cols = [col for col in right_df.columns if col != clave_right]\n",
    "    resultado[merged_cols] = None  # Inicializa columnas vacías\n",
    "\n",
    "    sin_match = pd.Series([True] * len(resultado))\n",
    "\n",
    "    for clave in claves_left:\n",
    "        temp_merge = left_df[sin_match].merge(right_df, left_on=clave, right_on=clave_right, how='left')\n",
    "        for col in merged_cols:\n",
    "            resultado.loc[sin_match, col] = temp_merge[col].values\n",
    "        sin_match = resultado[merged_cols[0]].isna()  # puedes mejorar esto si quieres validar múltiples columnas\n",
    "\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de76e5184c5a7b41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.622434Z",
     "start_time": "2025-05-15T18:30:43.603761Z"
    }
   },
   "outputs": [],
   "source": [
    "df_match_reparto = merge_por_reparto(df_match_oe, cat_rep, ['Ruta ZDD', 'Ruta ZVE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "103a2ff0ba2530fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.727218Z",
     "start_time": "2025-05-15T18:30:43.640646Z"
    }
   },
   "outputs": [],
   "source": [
    "df_biblia_full = df_match_reparto\n",
    "df_biblia_full.to_csv('Planner_Biblia_{}.csv'.format(UO), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da6a45a00beb971d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.823997Z",
     "start_time": "2025-05-15T18:30:43.745403Z"
    }
   },
   "outputs": [],
   "source": [
    "df_biblia_tradicional = df_biblia_full[df_biblia_full['Tipo'].isin(['PR', 'PCOP', 'PROP'])]\n",
    "df_biblia_tradicional.to_csv('Planner_Biblia_Tradicional_{}.csv'.format(UO), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1adab318d0f944d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.846200Z",
     "start_time": "2025-05-15T18:30:43.840901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID_VP    RUTA  ID_SAP\n",
      "209  60023555.0  QUWA05     712\n",
      "212  60023558.0  QUWA11     623\n",
      "213  60023562.0  QUWA14     785\n",
      "215  60023564.0  QUWA22     783\n",
      "216  60023565.0  QUWA24     652\n",
      "217  60023566.0  QUWA35     539\n",
      "223  60027573.0  QUWA03     741\n",
      "226  60027576.0  QUWA02     781\n",
      "227  60027577.0  QUWA01     760\n",
      "228  60027578.0  QUWA13     642\n",
      "229  60027579.0  QUWA04     777\n"
     ]
    }
   ],
   "source": [
    "df_vp_report = planner.pivot_table(index=['ID_VP','RUTA'], values='ID_SAP', aggfunc='count')\n",
    "df_vp_report = df_vp_report.reset_index()\n",
    "df_vp_report['ID_VP'] = df_vp_report['ID_VP'].astype(str)\n",
    "\n",
    "print(df_vp_report[df_vp_report['ID_SAP']>500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cceed6c3a63daf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T18:30:43.863889Z",
     "start_time": "2025-05-15T18:30:43.862777Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86114637-aa77-446c-84b5-c52666e0b7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
